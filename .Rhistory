}
# Test your analyze_applicant1() function
# Should be accepted
analyze_applicant1(student1_grades)
analyze_applicant1 <- function(grades) {
class1 <-  grades[1]# Index the first item in the grades vector
class2 <-  grades[2]# Index the second item in the grades vector
overall_gpa <- grades[3]# Index the third item in the grades vector
# Add an if / else conditional here
if (mean(c(class1,class2))>3.5 & overall_gpa>3.5)
print("The applicant should move on to the interview stage.")
else {
print("The applicant should be rejected.")
}
}
# Test your analyze_applicant1() function
# Should be accepted
analyze_applicant1(student1_grades)
# Test your analyze_applicant1() function
# Should be rejected
analyze_applicant1(student2_grades)
#Student 3
analyze_applicant1(student3_grades)
analyze_applicant2 <- function(grades) {
class1 <- grades[1] # Index the first item in the grades vector
class2 <-  grades[2]# Index the second item in the grades vector
overall_gpa <- grades[3] # Index the third item in the grades vector
# Add an if / else conditional here
if(class1<3.0|class2<3.0|overall_gpa<3.0)
print("The applicant should be rejected.")
else{
print("The applicant should move on to the interview stage")
}
}
# Test your analyze_applicant2() function
# Should be accepted
analyze_applicant2(student3_grades)
# Should be rejected
analyze_applicant2(student2_grades)
analyze_applicant3 <- function(grades) {
class1 <- grade[1]# Index the first item in the grades vector
class2 <-  grade[2]# Index the second item in the grades vector
overall_gpa <- grade[3]# Index the third item in the grades vector
# Add an if / else conditional here
if(class1==4.0|class2==4.0|overall_gpa==4.0)
print("Should be accepted")
else{
print("should be rejected")
}
}
# Test your analyze_applicant3() function
# Should be rejected
analyze_applicant3(student3_grades)
analyze_applicant3 <- function(grades) {
class1 <- grades[1]# Index the first item in the grades vector
class2 <-  grades[2]# Index the second item in the grades vector
overall_gpa <- grades[3]# Index the third item in the grades vector
# Add an if / else conditional here
if(class1==4.0|class2==4.0|overall_gpa==4.0)
print("Should be accepted")
else{
print("should be rejected")
}
}
# Test your analyze_applicant3() function
# Should be rejected
analyze_applicant3(student3_grades)
# Should be accepted
analyze_applicant3(student2_grades)
revised_analyze_applicant <- function(grades){
class1=grades[1]
class2=grades[2]
overall_gpa=grades[3]
if(overall_gpa>=3.5 & (class1>=3.5|class2>=3.5))
print("Should be accepted")
else{
print("Should be rejected")
}
}
# Test your revised_analyze_applicant() function
# Student 1
revised_analyze_applicant(student1_grades)
# Student 2
revised_analyze_applicant(student2_grades)
# Student 3
revised_analyze_applicant(student3_grades)
install.packages(stringr)
install.packages("stringr")
library("stringr")
library(stringr)
paste(a,b)
a <- 5
b <- 6
paste(a,b)
paste0(a,b)
test <- "abcdefghi"
substr(test,2,5)
install.packages("styler")
install.packages("lintr")
# Add in the necessary code to make these names a vector and assign it to the variable below
attendees <-c( "Rachel SR", "Prof. Walsh, PhD", "Vera FR", "Prof. Hendry, PhD", "Lily JR", "Najma SR", "Prof. Baeten MD", "Prof. Chu, MD", "Li FR")
# Add in the necessary code to make these names a vector and assign it to the variable below
attendees <-c( "Rachel SR", "Prof. Walsh, PhD", "Vera FR", "Prof. Hendry, PhD", "Lily JR", "Najma SR", "Prof. Baeten MD", "Prof. Chu, MD", "Li FR")
# How many people attended this campus event? Find out by using a built-in R function
length(attendees)
# Extract the third value in the vector attendees
attendees[3]
# Add in the necessary code to make these names a vector and assign it to the variable below
attendees <-c( "Rachel SR", "Prof. Walsh, PhD", "Vera FR", "Prof. Hendry, PhD", "Lily JR", "Najma SR", "Prof. Baeten MD", "Prof. Chu, MD", "Li FR")
# Load the stringr library
install.packages("stringr")
install.packages("stringr")
# Load the stringr library
install.packages("stringr")
install.packages("stringr")
library(stringr)
library("stringr")
# Use a stringr function to find out how many professors were at the event
str_detect(attendees, "Prof.")
# First, create a Boolean vector (e.g. TRUE FALSE TRUE) that corresponds to each person in the attendees vector and whether or not they are a professor
str_detect(attendees, "Prof.")
# Now, use a built-in R function to count how many professors were at the event (i.e., how many TRUE values are in the Boolean vector)
sum(attendees)
# Now, use a built-in R function to count how many professors were at the event (i.e., how many TRUE values are in the Boolean vector)
sum(attendees[str_detect(attendees,"Prof.")])
# Now, use a built-in R function to count how many professors were at the event (i.e., how many TRUE values are in the Boolean vector)
sum(attendees(str_detect(attendees,"Prof.")))
# First, create a Boolean vector (e.g. TRUE FALSE TRUE) that corresponds to each person in the attendees vector and whether or not they are a professor
str_detect(attendees, "Prof.")
# Now, use a built-in R function to count how many professors were at the event (i.e., how many TRUE values are in the Boolean vector)
sum(attendees(str_detect(attendees,"Prof.")))
# Now, use a built-in R function to count how many professors were at the event (i.e., how many TRUE values are in the Boolean vector)
sum(str_detect(attendees,"Prof."))
# Now, use a different stringr function to find out the year or degree of each student/professor â€” i.e., extract the last 3 letters from every string in the attendees vector
str_sub(attendees,-3)
# Use another built-in R function to identify only the unique years/degrees (no repeats)
unique(str_sub(attendees,-3))
# Use another built-in R function to calculate how many total unique years/degrees there are
length(unique(str_sub(attendees,-3)))
# Install tidyverse
install.packages("tidyverse")
# Load dplyr
library(dplyr)
# Load the pocket size data from the Pudding
# Read more about the data here: https://github.com/the-pudding/data/blob/master/pockets/measurements.csv
pocket_df <- read.csv("https://github.com/the-pudding/data/raw/master/pockets/measurements.csv")
# Make a new dataframe with only the columns "brand" and "price" and save as new_df
new_df <- pocket_df%>%select("brand","price")
# Make a new dataframe with only the columns "brand" and "price" and save as new_df
new_df <- pocket_df%>%select(brand,price)
# Filter the dataframe for only women and save as the variable women_pockets
women_pockets <- pocket_df %>% filter(menWomen == "women")
# Filter the dataframe for only men and save as the variable men_pockets
men_pockets <- pocket_df %>% filter(menWomen == "men")
# Calculate the average front pocket size for women and save as w_avg_front
w_avg_front <- women_pockets %>% summarize(mean)
# Calculate the average front pocket size for women and save as w_avg_front
w_avg_front <- women_pockets %>% summarize(mean(maxHeightFront))
# Calculate the average front pocket size for women and save as w_avg_front
w_avg_front <- women_pockets %>% summarize(avg_front = mean(maxHeightFront))
# Calculate the average front pocket size for men and save as m_avg_front
w_avg_front <- men_pockets %>% summarize(avg_front = mean(maxHeightFront))
# Calculate the average front pocket size for women and save as w_avg_front
w_avg_front <- women_pockets %>% summarize(avg_front = mean(maxHeightFront))
# Calculate the average front pocket size for men and save as m_avg_front
m_avg_front <- men_pockets %>% summarize(avg_front = mean(maxHeightFront))
# Find the proportion between women's and men's front pocket sizes
# Divide w_avg_front / m_avg_front
w_avg_front/m_avg_front
# Load the data from the following URL
# https://raw.githubusercontent.com/melaniewalsh/Neat-Datasets/main/1979-2020-National-Park-Visits-By-State.csv
# Save as np_data
np_data <- read.csv("https://raw.githubusercontent.com/melaniewalsh/Neat-Datasets/main/1979-2020-National-Park-Visits-By-State.csv")
# Load the data from the following URL
# https://raw.githubusercontent.com/melaniewalsh/Neat-Datasets/main/1979-2020-National-Park-Visits-By-State.csv
# Save as np_data
np_data <- read.csv("https://raw.githubusercontent.com/melaniewalsh/Neat-Datasets/main/1979-2020-National-Park-Visits-By-State.csv")
View(np_data)
View(np_data)
# Add a new column "park_state" to the dataframe that combines the name of each National Park and the state in which it is located like so
# Olympic NP, WA
np_data %>% mutate("park_state" = ParkName,State)
# Exploring visits to National Parks through data
library(dplyr)
# Add a new column "park_state" to the dataframe that combines the name of each National Park and the state in which it is located like so
# Olympic NP, WA
np_data %>% mutate("park_state" = ParkName,State)
View(np_data)
second_string <- "WA"
# Add a new column "park_state" to the dataframe that combines the name of each National Park and the state in which it is located like so
# Olympic NP, WA
first_string <- "Olympic NP"
paste0(first_string, ", ", second_string)
# Add a new column "park_state" to the dataframe that combines the name of each National Park and the state in which it is located like so
# Olympic NP, WA
np_data %>% mutate(park_state = paste0(ParkName, ", ", State))
source("~/Desktop/info201/week-5-exercises-nishant120/National-Park-Data.R")
# Add a new column "park_state" to the dataframe that combines the name of each National Park and the state in which it is located like so
# Olympic NP, WA
np_data <- np_data %>% mutate(park_state = paste0(ParkName, ", ", State))
View(np_data)
# What is the single greatest number of Recreation Visits for any National Park in any year?
# Save this filtered row as max_visits_row
max_visits_row <- np_data %>% summarize(max = max(RecreationVisits))
View(max_visits_row)
max_visits_row2 <- np_data %>% filter(RecreationVisits = max(RecreationVisits))
max_visits_row2 <- np_data %>% filter(RecreationVisits == max(RecreationVisits))
View(max_visits_row2)
# Now "pull" only the max number of visits and save as max_visits
max_visits <- np_data %>% summarize(max = max(RecreationVisits)) %>% pull(max)
# What is the lowest number of Recreation Visits for any National Park in any year?
# Save this filtered row as min_visits_row
min_visits_row <- np_data %>% summarize(min = min(RecreationVisits))
View(min_visits_row)
min_visits_row2 <- np_data %>% filer(min == min(RecreationVisits))
min_visits_row2 <- np_data %>% filter(min == min(RecreationVisits))
min_visits_row2 <- np_data %>% filter(min = min(RecreationVisits))
min_visits_row2 <- np_data %>% filter(min == min(RecreationVisits))
min_visits_row2 <- np_data %>% filter(RecreationVisits == min(RecreationVisits))
View(min_visits_row2)
# Load the National Park data from the following URL
# https://raw.githubusercontent.com/melaniewalsh/Neat-Datasets/main/1979-2020-National-Park-Visits-By-State.csv
# Save as np_data
np_data <- read.csv("https://raw.githubusercontent.com/melaniewalsh/Neat-Datasets/main/1979-2020-National-Park-Visits-By-State.csv")
# Load the National Park data from the following URL
# https://raw.githubusercontent.com/melaniewalsh/Neat-Datasets/main/1979-2020-National-Park-Visits-By-State.csv
# Save as np_data
np_data <- read.csv("https://raw.githubusercontent.com/melaniewalsh/Neat-Datasets/main/1979-2020-National-Park-Visits-By-State.csv")
View(np-data)
View(np_data)
# Find the average number of visits for each National Park
# Save as avg_park_visits and View()
# What park has the most and least average visits?
# What patterns or surprises do you notice?
np_data <- np_data %>% summarize(avg_park_visits = mean(RecreationVisits))
View(np_data)
# Load the National Park data from the following URL
# https://raw.githubusercontent.com/melaniewalsh/Neat-Datasets/main/1979-2020-National-Park-Visits-By-State.csv
# Save as np_data
np_data <- read.csv("https://raw.githubusercontent.com/melaniewalsh/Neat-Datasets/main/1979-2020-National-Park-Visits-By-State.csv")
View(np_data)
# Find the average number of visits for each National Park
# Save as avg_park_visits and View()
# What park has the most and least average visits?
# What patterns or surprises do you notice?
np_data %>% summarize(avg_park_visits = mean(RecreationVisits))
# Load the National Park data from the following URL
# https://raw.githubusercontent.com/melaniewalsh/Neat-Datasets/main/1979-2020-National-Park-Visits-By-State.csv
# Save as np_data
np_data <- read.csv("https://raw.githubusercontent.com/melaniewalsh/Neat-Datasets/main/1979-2020-National-Park-Visits-By-State.csv", stringsAsFactors = FALSE)
# Find the average number of visits for each National Park
# Save as avg_park_visits and View()
# What park has the most and least average visits?
# What patterns or surprises do you notice?
avg_park_visits <- np_data %>% group_by(ParkName) %>% summarize(avg_park_visits = mean(RecreationVisits))
View(avg_park_visits)
# Find the average number of visits for each state
# Save as avg_state_visits and View()
# What state has the most and least average visits?
# What patterns or surprises do you notice?
avg_state_visits <-  np_data %>% group_by(State) %>% summarize(avg_state_visits = mean(RecreationVisits))
View(avg_state_visits)
# Find the average number of visits for each National Park AND state (group by two columns!)
# Save as avg_park__state_visits and View()
# What National Park has the most and least average visits?
# What patterns or surprises do you notice?
avg_park__state_visits <- np_data %>% group_by(Parkname, State) %>% summarize(avg_visits = mean(RecreationVisits))
# Find the average number of visits for each National Park AND state (group by two columns!)
# Save as avg_park__state_visits and View()
# What National Park has the most and least average visits?
# What patterns or surprises do you notice?
avg_park__state_visits <- np_data %>% group_by(ParkName, State) %>% summarize(avg_visits = mean(RecreationVisits))
View(avg_park__state_visits)
# Find the number of distinct parks for each state
# Save as distinct_parks
# Which state has the most national parks?
# What patterns or surprises do you notice?
distinct_parks <- np_data %>% group_by(State) %>% summarize(num_parks = n_distinct(ParkName))
# Now "pull" only the max number of visits and save as min_visits
min_visits <- np_data %>% summarize(min = min(RecreationVisits)) %>% pull(min)
# Load the data from the following URL
# https://github.com/melaniewalsh/Neat-Datasets/raw/main/Survivor-Viewers.csv
# Save as survivor_df
survivor_df <- read.csv("https://github.com/melaniewalsh/Neat-Datasets/raw/main/Survivor-Viewers.csv")
# Load the DPLYR library
library(dplyr)
# Calculate the average number of viewers for each season `avg_viewers`
avg_viewers <- survivor_df %>% group_by(season) %>%  summarize(avg_viewers = mean(viewers, na.rm=TRUE))
# For fun, let's make a plot of avg viewers over season number
plot(avg_viewers, type = 'b')
# Find the episode with the most number of viewers `most_views_row`
most_views_row <- survivor_df %>%  filter(viewers == max(viewers, na.rm=TRUE)) %>% pull(viewers)
# Find the episode with the most number of viewers, then pull the number of viewers and save it as a variable `most_views`
most_views <- survivor_df %>%
filter(viewers == max(viewers, na.rm=TRUE)) %>% pull(viewers)
# Install package/data
install.packages("nycflights13")
library("nycflights13")
library("dplyr")
flights <- flights
airlines <- airlines
airports <- airports
# What was the average departure delay in each month?
# Save this as a data frame `dep_delay_by_month`
# Hint: you'll have to perform a grouping operation then summarize your data
dep_delay_by_month <- flights %>% group_by(month) %>% summarize(delay = mean(dep_delay, na.rm = TRUE))
# If your above data frame contains just two columns (e.g., "month", and "delay" in that order), you can create a scatterplot by passing that data frame to the built-in `plot()` function
plot(dep_delay_by_month, type = 'b')
# Use `left_join()` to join the "flights" dataframe to the "airlines" dataframe, which has the airline information
flights_airlines <- left_join(flights, airlines)
# What was the average departure delay for each airline?
# Hint: you'll have to perform a grouping operation then summarize your data
dep_delay_by_airline <- flights_airlines %>%group_by(name) %>%summarize(delay = mean(dep_delay, na.rm = TRUE))
# 1 Loading data ------------------------------------------------------
# The NYT COVID data at the national, state, and county level originally comes from the New York Times GitHub page: https://github.com/nytimes/covid-19-data/:
# For this assignment, you should use functions from the DPLYR package and pipe operator (%>%) syntax to explore the datasets.
# You should save your answers in variable names listed in backtics, like so `national`
# NOTE: You will often be asked to pull() specific values from your analysis.
# 1.a Load the tidyverse package and the dplyr package
library(dplyr)
library(tidyverse)
# 1.b Load the *national level* data from the following URL into a variable called `national`
# https://github.com/melaniewalsh/Neat-Datasets/raw/main/us-national-covid-2023.csv
national <- read.csv("https://github.com/melaniewalsh/Neat-Datasets/raw/main/us-national-covid-2023.csv")
# 1.c Load the *state level* data from the following URL into a variable called `states`
# https://github.com/melaniewalsh/Neat-Datasets/raw/main/us-states-covid-2023.csv
states <- read.csv("https://github.com/melaniewalsh/Neat-Datasets/raw/main/us-states-covid-2023.csv")
View(states)
# 1.d Load the *county level* data from the following URL into a variable called `counties`
# NOTE: This is a large dataset. It may take 30-60 seconds to load.
# https://github.com/melaniewal.sh/Neat-Datasets/raw/main/us-counties-covid=2023.csv
counties <- read.csv("https://github.com/melaniewalsh/Neat-Datasets/raw/main/us-counties-covid-2023.csv")
View(counties)
# 1.e How many observations (rows) are in each dataset?
# Create `obs_national`, `obs_states`, `obs_counties`
obs_national <- nrow(national)
obs_states <- nrow(states)
obs_counties <- nrow(counties)
# 1.f How many features (columns) are there in each dataset?
# Create `num_features_national`, `num_features_states`, `num_features_counties`
num_features_national <- ncol(national)
num_features_states <- ncol(states)
num_features_counties <- ncol(counties)
# 2 Exploratory Analysis ----------------------------------------------------
# Reflection 1 (answer in the README.md file)
# Before actually calculating the total number of COVID cases and deaths, record your guesses for the following questions. (1 point)
# Guess: How many total COVID cases do you think there have been in the U.S.?
# Guess: How many total COVID-related deaths do you think there have been in the U.S.?
# Guess: Which state do you think has the highest number of COVID cases, and which state do you think has the lowest?
#done in readme file.
# 2.a How many total COVID cases have there been in the U.S. by the most recent date in the dataset? Make sure to pull() this number `total_us_cases`
total_us_cases <- national %>% summarize(sum_cases = sum(cases)) %>% pull(sum_cases)
# 2.b How many total COVID-related deaths have there been in the U.S. by the most recent date in the dataset? Make sure to pull() this number `total_us_deaths`
total_us_deaths <- national %>% summarize(sum_deaths = sum(deaths)) %>% pull(sum_deaths)
# 2.c Which state has had the highest number of COVID cases? Make sure to pull() this value `state_highest_cases`
state_highest_cases <- states %>% filter(cases == max(cases)) %>% pull(state)
# 2.d What is the highest number of cases in a state? Make sure to pull() this number `num_highest_state`
num_highest_state <- states %>% filter(cases == max(cases)) %>% pull(cases)
# 2.e Which state has the highest ratio of deaths to cases (deaths/cases), as of the most recent date? Make sure to pull() this value
# HINT: You may need to create a new column in order to do this. `state_highest_ratio`
states <- states %>% mutate(deaths_to_cases_ratio = deaths/cases)
state_highest_ratio <- states %>% filter(deaths_to_cases_ratio == max(deaths_to_cases_ratio)) %>% pull(state)
# 2.f Which state has had the fewest number of cases as of the most
# recent date? Make sure to pull() this value `state_lowest_cases`
state_lowest)cases <- state %>% filter(cases == min(cases))
state_highest_ratio <- states %>% filter(deaths_to_cases_ratio == max(deaths_to_cases_ratio)) %>% pull(state)
# 2.f Which state has had the fewest number of cases as of the most
# recent date? Make sure to pull() this value `state_lowest_cases`
state_lowest_cases <- states %>% filter(cases == min(cases)) %>% pull(state)
return state_lowest_cases
# 2.f Which state has had the fewest number of cases as of the most
# recent date? Make sure to pull() this value `state_lowest_cases`
state_lowest_cases <- states %>% filter(cases == min(cases)) %>% pull(state)
state_lowest_cases
state_highest_ratio <- states %>% filter(date == max(date)) filter(deaths_to_cases_ratio == max(deaths_to_cases_ratio)) %>% pull(state)
state_highest_ratio <- states %>% filter(date == max(date)) %>% filter(deaths_to_cases_ratio == max(deaths_to_cases_ratio)) %>% pull(state)
state_lowest_cases <- states %>% filter(date == max(date)) %>% filter(cases == min(cases)) %>% pull(state)
total_us_cases <- national %>% filter(date == max(date)) %>%  summarize(sum_cases = sum(cases)) %>% pull(sum_cases)
# 2.a How many total COVID cases have there been in the U.S. by the most recent date in the dataset? Make sure to pull() this number `total_us_cases`
total_us_cases <- national %>%summarize(sum_cases = sum(cases)) %>% pull(sum_cases)
View(states)
state_highest_ratio <- states %>% filter(date == max(date)) %>% filter(deaths_to_cases_ratio == max(deaths_to_cases_ratio)) %>% pull(state)
# 2.f Which state has had the fewest number of cases as of the most
# recent date? Make sure to pull() this value `state_lowest_cases`
state_lowest_cases <- states %>% filter(date == max(date)) %>%  filter(cases == min(cases)) %>% pull(state)
# 2.a How many total COVID cases have there been in the U.S. by the most recent date in the dataset? Make sure to pull() this number `total_us_cases`
total_us_cases <- national %>% filter(date == max(date)) %>% summarize(sum_cases = sum(cases)) %>% pull(sum_cases)
# 2.b How many total COVID-related deaths have there been in the U.S. by the most recent date in the dataset? Make sure to pull() this number `total_us_deaths`
total_us_deaths <- national %>% filter(filter(date == max(date)) %>% summarize(sum_deaths = sum(deaths)) %>% pull(sum_deaths)
total_us_deaths <- national %>% filter(date == max(date)) %>% summarize(sum_deaths = sum(deaths)) %>% pull(sum_deaths)
total_us_deaths <- national %>% filter(date == max(date)) %>% summarize(sum_deaths = sum(deaths)) %>% pull(sum_deaths)
total_us_deaths <- national %>% filter(date == max(date)) %>% summarize(sum_deaths = sum(deaths)) %>% pull(sum_deaths)
# 2.a How many total COVID cases have there been in the U.S. by the most recent date in the dataset? Make sure to pull() this number `total_us_cases`
total_us_cases <- national %>% filter(date == max(date)) %>% summarize(sum_cases = sum(cases)) %>% pull(sum_cases)
county_highest_cases <- counties %>% filter(cases %>% max(cases)) %>% pull(county)
county_highest_cases <- counties %>% filter(cases == max(cases)) %>% pull(county)
# 2.h What is the highest number of cases that have happened in a single county? Make sure to pull() this number `num_highest_cases_county`
num_highest_cases_county <- counties %>% filter(cases == max(cases)) %>% pull(cases)
counties <- counties %>% mutate(location = paste0(county, ",", " ", state)
View(counties)
View(counties)
counties <- counties %>% mutate(location = paste0(county, ",", " ", state)
View(counties)
counties <- counties %>% mutate(location = paste0(county, ",", " ", state)
View(counties)
View(counties)
counties <-  counties %>% mutate(location = paste0(county, ",", " ", state))
View(counties)
location_most_deaths <- counties %>% filter(cases == max(cases)) %>% pull(location)
location_most_deaths <- counties %>% filter(deaths == max(deaths)) %>% pull(location)
location_most_deaths <- counties %>% filter(deaths == max(deaths)) %>% pull(location)
location_most_deaths <- counties %>% filter(deaths == max(deaths, na.rm = TRUE)) %>% pull(location)
national <- national %>% mutate(new_cases = cases - lag(cases))
# 2.l Similarly, the `deaths` columns is *not* the number of new deaths per day.
# Add  a new column to the `national` data frame called `new_deaths` that has the number of *new* deaths each day.
# HINT: The dyplr lag() function will be very helpful here.
national <- national %>% mutate(new_deaths = cases - lag(deaths))
# 2.l Similarly, the `deaths` columns is *not* the number of new deaths per day.
# Add  a new column to the `national` data frame called `new_deaths` that has the number of *new* deaths each day.
# HINT: The dyplr lag() function will be very helpful here.
national <- national %>% mutate(new_deaths = deaths - lag(deaths))
date_most_cases <- national %>% filter(new_cases == max(new_cases)) %>% pull(date)
date_most_cases <- national %>% filter(new_cases == max(new_cases, na.rm = TRUE)) %>% pull(date)
date_most_deaths <- national %>% filter(new_deaths == max(new_deaths, na.rm = TRUE)) %>% pull(date)
most_deaths <- national %>% filter(new_deaths == max(new_deaths, na.rm = TRUE)) %>% pull(deaths)
# You can plot this data with built-in plot functions
cases_plot <- plot(national$new_cases)
deaths_plot <- plot(national$new_deaths)
highest_case_in_each_state <- counties  %>% group_by(state) %>% filter(date == max(date)) %>%filter(cases == max(cases)) %>% pull(location)
# 3.a For each state, what is the county with the highest number of COVID cases? Make a dataframe that has every state and the county with the highest number of cases and corresponding rows (hint: you will need to use a grouping operation and a filter)
# Save as `highest_cases_in_each_state`
highest_case_in_each_state <- counties   %>% filter(date == max(date)) %>%group_by(state) %>% filter(cases == max(cases)) %>% pull(location)
# 3.a For each state, what is the county with the highest number of COVID cases? Make a dataframe that has every state and the county with the highest number of cases and corresponding rows (hint: you will need to use a grouping operation and a filter)
# Save as `highest_cases_in_each_state`
highest_case_in_each_state <- counties  %>% group_by(state) %>% filter(date == max(date)) %>% filter(cases == max(cases)) %>% pull(location)
# Reflection 3 (answer in README.md file)
# Inspect the `highest_cases_in_each_state` dataframe
View(highest_case_in_each_state)
# Reflection 3 (answer in README.md file)
# Inspect the `highest_cases_in_each_state` dataframe
highest_case_in_each_state
# 3.a For each state, what is the county with the highest number of COVID cases? Make a dataframe that has every state and the county with the highest number of cases and corresponding rows (hint: you will need to use a grouping operation and a filter)
# Save as `highest_cases_in_each_state`
highest_cases_in_each_state <- counties  %>% group_by(state) %>% filter(date == max(date)) %>% filter(cases == max(cases)) %>% pull(location)
# Reflection 3 (answer in README.md file)
# Inspect the `highest_cases_in_each_state` dataframe
highest_cases_in_each_state
lowest_deaths_in_each_state <- counties  %>% group_by(state) %>% filter(date == max(date)) %>% filter(deaths == min(deaths)) %>% pull(location)
# 4.a Create a `total_cases_counties` dataframe that adds up all the COIVD cases for all the counties for every date in the counties dataframe.
# You should name the columns `date` and `county_total_cases`.
total_cases_counties <- counties %>% group_by(date) %>% summarize(county_total_cases = sum(cases))
View(total_cases_counties)
# 4.b Join `total_cases_counties` with the `national` dataframe.
# Save this dataframe as `all_totals`.
all_totals <- left_join(total_cases_counties, national, by = "date")
View(all_totals)
# 4.c Filter the all_totals dataframe to find only the rows where the "county_total_cases" column does not match the "cases" column
# Save as national_county_diff
national_county_diff <- all_totals %>% filter(county_total_cases != cases)
View(national_county_diff)
# 4.d Calculate the number of rows in the national_county_diff dataframe
# Save as num_national_county_diff
num_national_county_diff <- nrow(national_county_diff)
# Reflection 3 (answer in README.md file)
# Inspect the `highest_cases_in_each_state` dataframe
highest_cases_in_each_state
# 4.d Calculate the number of rows in the national_county_diff dataframe
# Save as num_national_county_diff
num_national_county_diff <- nrow(national_county_diff)
# 4.d Calculate the number of rows in the national_county_diff dataframe
# Save as num_national_county_diff
num_national_county_diff <- nrow(national_county_diff)
national_county_diff <- national_county_diff %>% mutate(difference == cases - county_total_cases)
national_county_diff <- national_county_diff %>% mutate("difference" == cases - county_total_cases)
View(national_county_diff)
View(national_county_diff)
national_county_diff <- national_county_diff %>% mutate(difference = cases - county_total_cases)
View(national_county_diff)
View(national_county_diff)
national_county_diff %>% select(-'column name with quotes')
library("dplyr")
library("ggplot2")
office_df <- read.csv("https://raw.githubusercontent.com/melaniewalsh/Neat-Datasets/main/TheOfficeIMDBPerEpisode.csv")
# convert column to date format
office_df$AirDate<- as.Date(office_df$AirDate)
# What is the episode(s) with the highest IMDB rating in the dataset?
# Filter the data and save the row(s) as highest_rating_rows
highest_rating_rows <- office_df %>% filter(Rating == max(Rating, na.rm = TRUE))
# What is the episode(s) with the lowest IMDB rating in the dataset?
# Filter the data and save the row(s) as lowest_rating_rows
lowest_rating_rows <- office_df %>% filter(Rating == min(Rating, na.rm = TRUE))
# Which season of The Office was the best?
# Calculate the average IMDB rating for *each season*
# Save as rating_per_season
rating_per_season <- office_df %>% group_by(Season) %>% summarize(avg_rating = mean(Rating, na.rm = TRUE))
# Plot the IMDB rating for every episode of The Office as a line plot
# with date on the X axis and IMDB rating on the Y axis
ggplot(office_df) + geom_line(aes(x = AirDate, y = Rating))
# Now plot the *average* IMDB rating for *each season* as a scatterplot, a line plot, and both
ggplot(rating_per_season) + geom_line(aes(x = Season, y = avg_rating))
home_owner <- read.csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2021/2021-02-09/home_owner.csv')
race_wealth <- read.csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2021/2021-02-09/race_wealth.csv')
# Load relevant libraries
library(ggplot2)
total_avg_wealth <- race_wealth %>% filter(type == 'Average') %>% group_by(year) %>% summarize(avg_wealth = mean(wealth_family, na.rm=TRUE))
ggplot(data = total_avg_wealth) + geom_point(mapping = aes(x = year, y = avg_wealth)) + geom_line(mapping = aes(x = year, y = avg_wealth))
ggplot()
ggplot(data = total_avg_wealth) + geom_point(mapping = aes(x = year, y = avg_wealth)) + geom_line(mapping = aes(x = year, y = avg_wealth))
ggplot()
avg_race_wealth <- race_wealth %>% filter(type == 'Average')
ggplot(data = avg_race_wealth) + geom_line(mapping = aes(x = year, y = wealth_family, color = race))
ggplot(data = home_owner) + geom_line(mapping = aes(x = year, y = home_owner_pct, color = race))
